{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train \"GLaDOS\" Wake Word Model for openWakeWord\n",
    "\n",
    "This notebook trains a custom wake word model for the phrase **\"GLaDOS\"** (inspired by Portal).\n",
    "\n",
    "**Steps:**\n",
    "1. Install dependencies\n",
    "2. Generate synthetic speech using Piper TTS\n",
    "3. Download negative samples (music, noise, speech)\n",
    "4. Compute audio embeddings\n",
    "5. Train the model\n",
    "6. Export to ONNX\n",
    "\n",
    "**Runtime:** Use GPU runtime for faster training (Runtime → Change runtime type → T4 GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages with compatible versions for CUDA 12.6\n# Note: After running this cell, restart the runtime (Runtime → Restart runtime)\n\n# Install PyTorch 2.8.0 with CUDA 12.6 support (torchaudio 2.8 still has list_audio_backends)\n!pip install -q torch==2.8.0 torchaudio==2.8.0 --index-url https://download.pytorch.org/whl/cu126\n\n# Install speechbrain\n!pip install -q speechbrain\n\n# Install remaining dependencies\n!pip install -q openwakeword datasets scipy matplotlib tqdm\n\n# Install piper-tts for synthetic speech generation\n!pip install -q piper-tts\n\nprint(\"Installation complete! Now restart the runtime: Runtime → Restart runtime\")"
  },
  {
   "cell_type": "code",
   "source": "# Restart runtime after installation (run this cell, then continue from the next cell)\n# This is required to properly load the new package versions\n\nimport os\nos.kill(os.getpid(), 9)  # This will restart the Colab runtime",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import collections\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from numpy.lib.format import open_memmap\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import scipy.io.wavfile\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import IPython.display as ipd\n",
    "\n",
    "import openwakeword\n",
    "import openwakeword.data\n",
    "import openwakeword.utils\n",
    "import datasets\n",
    "\n",
    "# Configuration\n",
    "WAKE_WORD = \"glados\"\n",
    "WAKE_PHRASES = [\n",
    "    \"GLaDOS\",\n",
    "    \"Hey GLaDOS\",\n",
    "    \"Okay GLaDOS\",\n",
    "    \"Hi GLaDOS\"\n",
    "]\n",
    "CLIP_LENGTH_SECONDS = 3  # Window size for model\n",
    "N_SYNTHETIC_PER_PHRASE = 500  # Clips per phrase per voice\n",
    "\n",
    "print(f\"Training wake word model for: {WAKE_WORD}\")\n",
    "print(f\"Phrases: {WAKE_PHRASES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic Speech with Piper TTS\n",
    "\n",
    "We'll generate diverse synthetic examples using multiple Piper TTS voices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Piper TTS voices (English, multiple speakers)\n",
    "!mkdir -p piper_voices\n",
    "\n",
    "PIPER_VOICES = [\n",
    "    (\"en_US-lessac-medium\", \"https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_US/lessac/medium/en_US-lessac-medium.onnx\"),\n",
    "    (\"en_US-libritts-high\", \"https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_US/libritts/high/en_US-libritts-high.onnx\"),\n",
    "    (\"en_US-amy-medium\", \"https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_US/amy/medium/en_US-amy-medium.onnx\"),\n",
    "    (\"en_GB-alba-medium\", \"https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_GB/alba/medium/en_GB-alba-medium.onnx\"),\n",
    "    (\"en_GB-aru-medium\", \"https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_GB/aru/medium/en_GB-aru-medium.onnx\"),\n",
    "]\n",
    "\n",
    "for voice_name, url in PIPER_VOICES:\n",
    "    onnx_path = f\"piper_voices/{voice_name}.onnx\"\n",
    "    json_path = f\"piper_voices/{voice_name}.onnx.json\"\n",
    "    if not os.path.exists(onnx_path):\n",
    "        print(f\"Downloading {voice_name}...\")\n",
    "        !wget -q -O {onnx_path} {url}\n",
    "        !wget -q -O {json_path} {url}.json\n",
    "    else:\n",
    "        print(f\"{voice_name} already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic speech clips\n",
    "import json\n",
    "import struct\n",
    "import random\n",
    "\n",
    "os.makedirs(f\"{WAKE_WORD}_clips\", exist_ok=True)\n",
    "\n",
    "def generate_piper_clips(voice_path, phrases, n_per_phrase, output_dir):\n",
    "    \"\"\"Generate synthetic speech clips with Piper TTS.\"\"\"\n",
    "    voice_name = os.path.basename(voice_path).replace(\".onnx\", \"\")\n",
    "    clips_generated = 0\n",
    "    \n",
    "    for phrase in phrases:\n",
    "        for i in range(n_per_phrase):\n",
    "            # Vary speech rate and noise scale for diversity\n",
    "            length_scale = random.uniform(0.8, 1.2)  # Speed variation\n",
    "            noise_scale = random.uniform(0.3, 0.7)   # Voice variation\n",
    "            \n",
    "            output_file = f\"{output_dir}/{voice_name}_{phrase.replace(' ', '_')}_{i:04d}.wav\"\n",
    "            \n",
    "            if os.path.exists(output_file):\n",
    "                clips_generated += 1\n",
    "                continue\n",
    "            \n",
    "            # Use piper CLI to generate audio\n",
    "            cmd = f'echo \"{phrase}\" | piper --model {voice_path} --length_scale {length_scale} --noise_scale {noise_scale} --output_file {output_file} 2>/dev/null'\n",
    "            os.system(cmd)\n",
    "            \n",
    "            # Convert to 16kHz if needed\n",
    "            if os.path.exists(output_file):\n",
    "                clips_generated += 1\n",
    "    \n",
    "    return clips_generated\n",
    "\n",
    "total_clips = 0\n",
    "for voice_name, _ in tqdm(PIPER_VOICES, desc=\"Generating clips per voice\"):\n",
    "    voice_path = f\"piper_voices/{voice_name}.onnx\"\n",
    "    n = generate_piper_clips(voice_path, WAKE_PHRASES, N_SYNTHETIC_PER_PHRASE, f\"{WAKE_WORD}_clips\")\n",
    "    total_clips += n\n",
    "    print(f\"  {voice_name}: {n} clips\")\n",
    "\n",
    "print(f\"\\nTotal synthetic clips generated: {total_clips}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample all clips to 16kHz (required by openWakeWord)\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "\n",
    "def resample_to_16k(input_path, output_path=None):\n",
    "    \"\"\"Resample audio file to 16kHz.\"\"\"\n",
    "    if output_path is None:\n",
    "        output_path = input_path\n",
    "    \n",
    "    try:\n",
    "        sr, data = wavfile.read(input_path)\n",
    "        if sr != 16000:\n",
    "            # Resample\n",
    "            n_samples = int(len(data) * 16000 / sr)\n",
    "            data = signal.resample(data, n_samples).astype(np.int16)\n",
    "            wavfile.write(output_path, 16000, data)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error resampling {input_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Resample all generated clips\n",
    "clips = list(Path(f\"{WAKE_WORD}_clips\").glob(\"*.wav\"))\n",
    "for clip in tqdm(clips, desc=\"Resampling to 16kHz\"):\n",
    "    resample_to_16k(str(clip))\n",
    "\n",
    "print(f\"Resampled {len(clips)} clips to 16kHz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listen to a sample clip\n",
    "sample_clips = list(Path(f\"{WAKE_WORD}_clips\").glob(\"*.wav\"))[:5]\n",
    "for clip in sample_clips:\n",
    "    print(f\"Playing: {clip.name}\")\n",
    "    ipd.display(ipd.Audio(str(clip), rate=16000, autoplay=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download Negative Data\n",
    "\n",
    "Download samples of music, noise, and speech that do NOT contain the wake word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download negative data samples\n",
    "!wget -q -O fma_sample.zip \"https://f002.backblazeb2.com/file/openwakeword-resources/data/fma_sample.zip\"\n",
    "!unzip -q -o fma_sample.zip\n",
    "\n",
    "!wget -q -O fsd50k_sample.zip \"https://f002.backblazeb2.com/file/openwakeword-resources/data/fsd50k_sample.zip\"\n",
    "!unzip -q -o fsd50k_sample.zip\n",
    "\n",
    "print(\"Downloaded FMA (music) and FSD50K (noise) samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Common Voice speech samples\n",
    "cv_11 = datasets.load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\", split=\"test\", streaming=True, trust_remote_code=True)\n",
    "cv_11 = cv_11.cast_column(\"audio\", datasets.Audio(sampling_rate=16000, mono=True))\n",
    "cv_11 = iter(cv_11)\n",
    "\n",
    "# Convert and save clips (first 5000)\n",
    "os.makedirs(\"cv11_test_clips\", exist_ok=True)\n",
    "limit = 5000\n",
    "\n",
    "for i in tqdm(range(limit), desc=\"Downloading Common Voice clips\"):\n",
    "    try:\n",
    "        example = next(cv_11)\n",
    "        output = os.path.join(\"cv11_test_clips\", example[\"path\"][0:-4] + \".wav\")\n",
    "        os.makedirs(os.path.dirname(output), exist_ok=True)\n",
    "        wav_data = (example[\"audio\"][\"array\"] * 32767).astype(np.int16)\n",
    "        scipy.io.wavfile.write(output, 16000, wav_data)\n",
    "    except StopIteration:\n",
    "        break\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "print(f\"Downloaded Common Voice clips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute Audio Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create audio feature extractor\n",
    "F = openwakeword.utils.AudioFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get negative clips\n",
    "negative_clips, negative_durations = openwakeword.data.filter_audio_paths(\n",
    "    [\"fma_sample\", \"fsd50k_sample\", \"cv11_test_clips\"],\n",
    "    min_length_secs=1.0,\n",
    "    max_length_secs=60*30,\n",
    "    duration_method=\"header\"\n",
    ")\n",
    "\n",
    "print(f\"{len(negative_clips)} negative clips, ~{sum(negative_durations)//3600} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute negative features\n",
    "audio_dataset = datasets.Dataset.from_dict({\"audio\": negative_clips})\n",
    "audio_dataset = audio_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000))\n",
    "\n",
    "batch_size = 64\n",
    "clip_size = CLIP_LENGTH_SECONDS\n",
    "N_total = int(sum(negative_durations) // clip_size)\n",
    "n_feature_cols = F.get_embedding_shape(clip_size)\n",
    "\n",
    "output_file = \"negative_features.npy\"\n",
    "output_array_shape = (N_total, n_feature_cols[0], n_feature_cols[1])\n",
    "fp = open_memmap(output_file, mode='w+', dtype=np.float32, shape=output_array_shape)\n",
    "\n",
    "row_counter = 0\n",
    "for i in tqdm(np.arange(0, audio_dataset.num_rows, batch_size), desc=\"Computing negative features\"):\n",
    "    wav_data = [(j[\"array\"] * 32767).astype(np.int16) for j in audio_dataset[i:i+batch_size][\"audio\"]]\n",
    "    wav_data = openwakeword.data.stack_clips(wav_data, clip_size=16000*clip_size).astype(np.int16)\n",
    "    features = F.embed_clips(x=wav_data, batch_size=1024, ncpu=2)\n",
    "    \n",
    "    if row_counter + features.shape[0] > N_total:\n",
    "        fp[row_counter:min(row_counter+features.shape[0], N_total), :, :] = features[0:N_total - row_counter, :, :]\n",
    "        fp.flush()\n",
    "        break\n",
    "    else:\n",
    "        fp[row_counter:row_counter+features.shape[0], :, :] = features\n",
    "        row_counter += features.shape[0]\n",
    "        fp.flush()\n",
    "\n",
    "openwakeword.data.trim_mmap(output_file)\n",
    "print(f\"Saved negative features to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get positive clips\n",
    "positive_clips, durations = openwakeword.data.filter_audio_paths(\n",
    "    [f\"{WAKE_WORD}_clips\"],\n",
    "    min_length_secs=0.3,\n",
    "    max_length_secs=3.0,\n",
    "    duration_method=\"header\"\n",
    ")\n",
    "\n",
    "print(f\"{len(positive_clips)} positive clips after filtering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mix positive clips with background noise and compute features\n",
    "sr = 16000\n",
    "total_length = int(sr * CLIP_LENGTH_SECONDS)\n",
    "\n",
    "# Position positive clips to end near the end of the window (for low latency detection)\n",
    "jitters = (np.random.uniform(0, 0.2, len(positive_clips)) * sr).astype(np.int32)\n",
    "starts = [total_length - (int(np.ceil(d * sr)) + j) for d, j in zip(durations, jitters)]\n",
    "\n",
    "# Create mixing generator\n",
    "batch_size = 8\n",
    "mixing_generator = openwakeword.data.mix_clips_batch(\n",
    "    foreground_clips=positive_clips,\n",
    "    background_clips=negative_clips,\n",
    "    combined_size=total_length,\n",
    "    batch_size=batch_size,\n",
    "    snr_low=5,\n",
    "    snr_high=15,\n",
    "    start_index=starts,\n",
    "    volume_augmentation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listen to a mixed sample\n",
    "mixed_clips, labels, background_clips = next(mixing_generator)\n",
    "print(\"Sample mixed clip (wake word + background noise):\")\n",
    "ipd.display(ipd.Audio(mixed_clips[0], rate=16000, normalize=True, autoplay=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate generator and compute positive features\n",
    "mixing_generator = openwakeword.data.mix_clips_batch(\n",
    "    foreground_clips=positive_clips,\n",
    "    background_clips=negative_clips,\n",
    "    combined_size=total_length,\n",
    "    batch_size=batch_size,\n",
    "    snr_low=5,\n",
    "    snr_high=15,\n",
    "    start_index=starts,\n",
    "    volume_augmentation=True,\n",
    ")\n",
    "\n",
    "N_total = len(positive_clips)\n",
    "n_feature_cols = F.get_embedding_shape(CLIP_LENGTH_SECONDS)\n",
    "\n",
    "output_file = f\"{WAKE_WORD}_features.npy\"\n",
    "output_array_shape = (N_total, n_feature_cols[0], n_feature_cols[1])\n",
    "fp = open_memmap(output_file, mode='w+', dtype=np.float32, shape=output_array_shape)\n",
    "\n",
    "row_counter = 0\n",
    "for batch in tqdm(mixing_generator, total=N_total//batch_size, desc=\"Computing positive features\"):\n",
    "    batch_data, lbls, background = batch[0], batch[1], batch[2]\n",
    "    features = F.embed_clips(batch_data, batch_size=256)\n",
    "    fp[row_counter:row_counter+features.shape[0], :, :] = features\n",
    "    row_counter += features.shape[0]\n",
    "    fp.flush()\n",
    "    \n",
    "    if row_counter >= N_total:\n",
    "        break\n",
    "\n",
    "openwakeword.data.trim_mmap(output_file)\n",
    "print(f\"Saved positive features to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features\n",
    "negative_features = np.load(\"negative_features.npy\")\n",
    "positive_features = np.load(f\"{WAKE_WORD}_features.npy\")\n",
    "\n",
    "print(f\"Negative features shape: {negative_features.shape}\")\n",
    "print(f\"Positive features shape: {positive_features.shape}\")\n",
    "\n",
    "X = np.vstack((negative_features, positive_features))\n",
    "y = np.array([0]*len(negative_features) + [1]*len(positive_features)).astype(np.float32)[..., None]\n",
    "\n",
    "print(f\"\\nTotal samples: {len(X)}\")\n",
    "print(f\"Positive ratio: {y.sum()/len(y)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch dataloader\n",
    "batch_size = 512\n",
    "training_data = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(torch.from_numpy(X), torch.from_numpy(y)),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "layer_dim = 64  # Slightly larger for better accuracy\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(X.shape[1] * X.shape[2], layer_dim),\n",
    "    nn.LayerNorm(layer_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(layer_dim, layer_dim),\n",
    "    nn.LayerNorm(layer_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(layer_dim, 1),\n",
    "    nn.Sigmoid(),\n",
    ").to(device)\n",
    "\n",
    "loss_function = torch.nn.functional.binary_cross_entropy\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "n_epochs = 15\n",
    "history = collections.defaultdict(list)\n",
    "\n",
    "for epoch in tqdm(range(n_epochs), desc=\"Training\"):\n",
    "    epoch_loss = []\n",
    "    epoch_recall = []\n",
    "    \n",
    "    for batch in training_data:\n",
    "        x, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "        \n",
    "        # Weight negative class higher to reduce false positives\n",
    "        weights = torch.ones(y_batch.shape[0], device=device)\n",
    "        weights[y_batch.flatten() == 1] = 0.1\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x)\n",
    "        loss = loss_function(predictions, y_batch, weights[..., None])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss.append(float(loss.detach().cpu().numpy()))\n",
    "        \n",
    "        # Calculate recall\n",
    "        tp = sum(predictions.flatten()[y_batch.flatten() == 1] >= 0.5)\n",
    "        fn = sum(predictions.flatten()[y_batch.flatten() == 1] < 0.5)\n",
    "        if (tp + fn) > 0:\n",
    "            epoch_recall.append(float((tp / (tp + fn)).detach().cpu().numpy()))\n",
    "    \n",
    "    history['loss'].append(np.mean(epoch_loss))\n",
    "    history['recall'].append(np.mean(epoch_recall))\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}: Loss={history['loss'][-1]:.4f}, Recall={history['recall'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training metrics\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['loss'], label='Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['recall'], label='Recall', color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Training Recall')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to ONNX format\n",
    "model.eval()\n",
    "model_cpu = model.cpu()\n",
    "\n",
    "output_path = f\"{WAKE_WORD}.onnx\"\n",
    "dummy_input = torch.zeros((1, X.shape[1], X.shape[2]))\n",
    "\n",
    "torch.onnx.export(\n",
    "    model_cpu,\n",
    "    dummy_input,\n",
    "    output_path,\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    ")\n",
    "\n",
    "print(f\"Model exported to: {output_path}\")\n",
    "print(f\"Model size: {os.path.getsize(output_path) / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with openWakeWord\n",
    "oww = openwakeword.Model(\n",
    "    wakeword_model_paths=[f\"{WAKE_WORD}.onnx\"],\n",
    "    enable_speex_noise_suppression=True,\n",
    "    vad_threshold=0.5\n",
    ")\n",
    "\n",
    "print(f\"Loaded model: {WAKE_WORD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a positive clip\n",
    "test_clip = list(Path(f\"{WAKE_WORD}_clips\").glob(\"*.wav\"))[0]\n",
    "print(f\"Testing on: {test_clip}\")\n",
    "\n",
    "scores = oww.predict_clip(str(test_clip))\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.plot([s[WAKE_WORD] for s in scores])\n",
    "plt.axhline(y=0.5, color='r', linestyle='--', label='Threshold')\n",
    "plt.xlabel('Frame (80ms each)')\n",
    "plt.ylabel('Score')\n",
    "plt.title(f'Wake Word Detection Score - {test_clip.name}')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "max_score = max([s[WAKE_WORD] for s in scores])\n",
    "print(f\"Max score: {max_score:.3f} {'✓ DETECTED' if max_score >= 0.5 else '✗ NOT DETECTED'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on multiple positive clips\n",
    "test_clips = list(Path(f\"{WAKE_WORD}_clips\").glob(\"*.wav\"))[:50]\n",
    "detected = 0\n",
    "\n",
    "for clip in tqdm(test_clips, desc=\"Testing positive clips\"):\n",
    "    scores = oww.predict_clip(str(clip))\n",
    "    max_score = max([s[WAKE_WORD] for s in scores])\n",
    "    if max_score >= 0.5:\n",
    "        detected += 1\n",
    "\n",
    "print(f\"\\nDetection rate: {detected}/{len(test_clips)} ({detected/len(test_clips)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test false positive rate on negative clips\n",
    "test_negative = negative_clips[:100]\n",
    "false_positives = 0\n",
    "\n",
    "for clip in tqdm(test_negative, desc=\"Testing negative clips\"):\n",
    "    try:\n",
    "        scores = oww.predict_clip(clip)\n",
    "        max_score = max([s[WAKE_WORD] for s in scores])\n",
    "        if max_score >= 0.5:\n",
    "            false_positives += 1\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(f\"\\nFalse positive rate: {false_positives}/{len(test_negative)} ({false_positives/len(test_negative)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Download the Model\n",
    "\n",
    "Download the trained model to use with your voice assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model file\n",
    "from google.colab import files\n",
    "\n",
    "print(f\"Downloading {WAKE_WORD}.onnx...\")\n",
    "files.download(f\"{WAKE_WORD}.onnx\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DONE! Next steps:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"1. Save {WAKE_WORD}.onnx to your Raspberry Pi\")\n",
    "print(f\"2. Update your WakeWordConfig:\")\n",
    "print(f\"   WakeWordConfig(model_path='path/to/{WAKE_WORD}.onnx')\")\n",
    "print(f\"3. Run your voice assistant and say 'GLaDOS'!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}